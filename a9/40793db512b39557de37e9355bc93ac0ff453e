From: wlc@jctaylor.com (William Corcoran)
Date: Tue, 17 Jan 2017 00:22:54 -0500
Subject: [TUHS] TUHS Digest, Vol 14, Issue 63
In-Reply-To: <4D79D86D-1A54-4E3E-AA5A-26A71AC42B43@superglobalmegacorp.com>
References: <mailman.1.1484532001.2693.tuhs@minnie.tuhs.org>
 <201701161600.v0GG00XA080461@tahoe.cs.Dartmouth.EDU>
 <20170116164421.GJ6647@mcvoy.com>
 <C35441DB-7A62-498A-B173-3EA78FB89B15@tfeb.org>
 <4D79D86D-1A54-4E3E-AA5A-26A71AC42B43@superglobalmegacorp.com>
Message-ID: <0FA4AF13-162F-4E78-9FB6-CDC4B07F97AE@jctaylor.com>

However, in the high transaction volume corporate world,  the FC card is peanuts and the Ethernet card is peanut shells.

A product's security defense is often said to be inversely proportional to its market share.  So, we chose FC over Ethernet primarily for this reason (not lack of market share, but for its purported security.)

The cost of an FC solution was absurd when compared to Ethernet by any rational and reasonable means.

Nevertheless, businesses that relied on these devices for larger volumes of financial transactions were led to believe that the NET cost of FC was far cheaper than Ethernet.

I complained to our vendor at the time that Ethernet speeds were eclipsing FC.  We were told that the FC fabric was far superior to Ethernet---especially its security.

Hogwash.  I remember racks of 2Gb FC switches, only three years old, completely and totally obsolete.

There are guys like me with lollipops on the cheeks----born every minute.




On Jan 16, 2017, at 11:42 PM, Jason Stevens <jsteve at superglobalmegacorp.com<mailto:jsteve at superglobalmegacorp.com>> wrote:

I only used FC when everyone was jumping onto the iSCSI bandwagon for 1gb NICs and you could get FC stuff on the cheap. I was using the Compaq MSA arrays with a built in FC switch, and using all like cards on like servers with the then "new" ESX 2.5 and it worked like a champ. I've always been a fan of separate storage networks but in the brave new world of virtual everything it really doesn't matter as more and more moves up the stack. I'm sure we will be on AWS in the next few years then in 10 years there will be the tick tock swing of moving processing into closets and then back to private data centres...

On January 17, 2017 7:41:16 AM GMT+08:00, Tim Bradshaw <tfb at tfeb.org<mailto:tfb at tfeb.org>> wrote:

Less than ten years ago I wrote a big rant at people where I worked about fibre channel: all our machines had two entirely different networks attached to them: one built on ethernet which was at that point all Gb on new machines and 10Gb on some (I don't think that 10Gb switches were really available yet though) & where you could stuff a machine with interfaces for the cost of a good meal, and where everything just talked to everything else ... and one built on fibre channel which might have been 2Gb, where an interface cost as much as a car, and where interoperability involved weeks pissing around with firmware in the cards, and sometimes just buying new ones.  Fibre channel was just laughably worse than ethernet.

No one listened, of course, because my political skills are akin to those of a goat, and fibre channel is *storage* which is completely different than networking, somehow.

Perhaps people still use fibre channel.

 On 16 Jan 2017, at 16:44, Larry McVoy <lm at mcvoy.com<mailto:lm at mcvoy.com>> wrote:

 I held up the two cards, disclosed the cost, and said "this ATM card is
 always going to be expensive but the ethernet card is gonna be $10 in
 a year or two.  Why?  Volume.  Every computer has ethernet, it's gonna
 do nothing but get cheaper.  And you're gonna see ethernet over fiber,
 long haul, you're going to see 100 Mbit, gigabit ethernet, and it's
 going to be cheap.  ATM is going nowhere."

--
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://minnie.tuhs.org/pipermail/tuhs/attachments/20170117/8bec6884/attachment-0001.html>

