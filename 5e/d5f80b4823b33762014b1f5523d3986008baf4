From: cowan@mercury.ccil.org (John Cowan)
Date: Sun, 31 Jan 2016 12:38:46 -0500
Subject: [TUHS] Short history of 'grep'
In-Reply-To: <56AE4029.7010701@mhorton.net>
References: <20160130030012.GB9762@minnie.tuhs.org>
 <56AD0AB7.40701@mhorton.net>
 <alpine.BSF.2.11.1601310639150.14724@aneurin.horsfall.org>
 <56AD1B28.4010908@mhorton.net>
 <alpine.BSF.2.11.1601310740170.14724@aneurin.horsfall.org>
 <CAOkr1zUprhZws1WoD+4fuepMZkzNLO9HjNfLp1fGrDnjx4JKtA@mail.gmail.com>
 <alpine.BSF.2.11.1601311227470.14724@aneurin.horsfall.org>
 <alpine.LRH.2.20.1601301802360.8927@waffle.shalott.net>
 <56AE4029.7010701@mhorton.net>
Message-ID: <20160131173846.GB7792@mercury.ccil.org>

Mary Ann Horton scripsit:

> When I tell this story to nontechical folks, I prefix it with the
> brief note that fgrep ought to be fastest, because it's simple, and
> egrep ought to be slowest, because it's complex, but in reality
> fgrep is slowest and egrep is fastest.  

Is it really?  The one time I used fgrep in production, I was checking
a a few hundred documents at a time to see which ones contained any of a
few thousand keywords.  "fgrep -l -f keywords" seemed to do the job quite
quickly: would it really have been faster to assemble the keywords into
a single egrep regex and use egrep?  (This was on Solaris, so using more
or less classic fgrep, not GNU grep.) For a while I referred to myself as
"just another desperate fgrep hacker".

I use "ex" as my normal text editor (including for this email); I drop
into vi mode occasionally, mostly to bounce on the % key when writing
Lisp.  Because there is no support for | in ex regexes, I rely on the
low entropy of English text (about 2.7 bits per letter) and search
for e.g. "open|shut" by searching for "[os][ph][eu][nt]".  I may
get a few false positives, but they will easily be removed by vgrep.

-- 
John Cowan          http://www.ccil.org/~cowan        cowan at ccil.org
After fixing the Y2K bug in an application:
        WELCOME TO <censored>
        DATE: MONDAK, JANUARK 1, 1900

