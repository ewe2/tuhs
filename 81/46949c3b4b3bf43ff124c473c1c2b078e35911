From: ron@ronnatalie.com (Ron Natalie)
Date: Tue, 21 Nov 2017 10:40:35 -0500
Subject: [TUHS] UNIX on S/370
In-Reply-To: <CAC20D2NGruq-8kBPjUmEmZEJZD8fVQs2ZcAHuV=qANjW1MzPog@mail.gmail.com>
References: <CAJfiPzxnwNdAHLRV4+QUF-k-ib4tuFj5BLy-r-jL9W7zHHziQw@mail.gmail.com>
 <201711200350.vAK3omwQ013495@freefriends.org>
 <CAC20D2Mtp1+fPP5W8TP8HRiu7EKJGfWPVADDRwydVwv2UEXheQ@mail.gmail.com>
 <05be01d36266$4b32e810$e198b830$@ronnatalie.com>
 <93a49f88-c9b2-d395-ba82-f7a3577bffea@tnetconsulting.net>
 <05ec01d3627a$6960cc80$3c226580$@ronnatalie.com>
 <374F4C47-30B2-4E3E-A493-62C259D25CF0@ccc.com>
 <f0329b7d-e5aa-8c94-d0da-f4086d41534f@tnetconsulting.net>
 <060301d362c0$493fd7c0$dbbf8740$@ronnatalie.com>
 <CAC20D2NGruq-8kBPjUmEmZEJZD8fVQs2ZcAHuV=qANjW1MzPog@mail.gmail.com>
Message-ID: <064601d362df$13425660$39c70320$@ronnatalie.com>

Yes, definitely more to TCF than just what I mentioned, I just wanted to indicate why you wouldn’t necessarily need to be connected to the 370-node directly in order to use it.

 

It was really handy (though somewhat confusing at times) when we were doing the 4 process i860 addition to the system.     It was far too easy to just let use an i386 instance of some application rather than providing the native one (since the W4 card was a microchannel card that went into the PS2).   Unless you tried really hard, you couldn’t tell if the app was running remotely.

                                

 

From: Clem Cole [mailto:clemc@ccc.com] 
Sent: Tuesday, November 21, 2017 8:52 AM
To: Ron Natalie
Cc: Grant Taylor; The Eunuchs Hysterical Society
Subject: Re: [TUHS] UNIX on S/370

 

 

 

On Tue, Nov 21, 2017 at 7:00 AM, Ron Natalie <ron at ronnatalie.com> wrote:


The other choice is to use TCF.   TCF allowed you to transparently run binaries on a remote machine and it took care of redirecting stdin/stdout back to your machine.

 

​It did a heck of lot more than that.   TCF made all the systems participating in the cluster one transparent machine - TCF - the Transparent Computing Facility.  Processes migrated between processors, for execution.   Your 'login session' was as Ron points out typically an x-windows/xterm.   But resources that you used over the course of the session could be anywhere within the cluster.    Nodes could come and go and the cluster survive.   You can read all about it in the Locus book which available from Amazon [MIT press - Popek and Walker].

 

The two biggest errors IMO was that the cluster size was screwed down to 32 and the kernel was ad hoc and very heavily hacked, so it was hard to put the features into other systems.

 

FYI:  the follow on to it, TNC (Transparent Network Computing); corrected both of those issues.  TNC becme the OS for Intel Paragon which scaled to 4096 nodes.   Locus moved the technology into 18 different components and made them available separately.   They were all eventually made open source.  The TNC file system became DEC's TruCluster FS, a project which I lead and brought me to DEC.   I had also lead a group in Boston that had put TNC into HP-UX with full process migration before we did the work for DEC actually, but HP cancelled the project before it ever shipped.  Bruce  and the west coast Locus folks put most of TNC into Linux a few years ago before he retired and as I say, succeeded to release it as open source -- I can dig up a URL for that project, if folks are interested.  I had it running on a small 8 node cluster about 8-10 years ago; it was very cool; but was using a older version of the RH and a 2.4 Linux kernel around the time Linux went to the the 2.6 kernel.

 

Clem

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://minnie.tuhs.org/pipermail/tuhs/attachments/20171121/ac00e5a5/attachment-0001.html>

