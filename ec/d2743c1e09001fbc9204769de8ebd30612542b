From: clemc@ccc.com (Clem Cole)
Date: Fri, 1 Sep 2017 16:42:49 -0400
Subject: [TUHS] Future Languages (was Pascal not Favorite...)
In-Reply-To: <e2b9a48d33d28cc42717092e6061146617d09620@webmail.yaccman.com>
References: <f4dbb2e0-6cbe-1450-cd25-a54d2ad0e822@telegraphics.com.au>
 <e2b9a48d33d28cc42717092e6061146617d09620@webmail.yaccman.com>
Message-ID: <CAC20D2M0NDH5h6Ja758dPEi244u3tMhsng0VkSg8a9Xvp=6vrg@mail.gmail.com>

On Fri, Sep 1, 2017 at 2:15 PM, Steve Johnson <scj at yaccman.com> wrote:

>
> I may just be a grumpy old fart, but I think programming languages today
> are holding us back.   Nearly all of them...
>
​
I don't disagree with the idea or intent, but I don't think it's technology
that is hold us back Steve, it is economics.​


   1. When I was at CMU in the 70s, my Profs told me Fortran was dead.  In
   2017, it is still is the #1 *production* programming language on the
   supercomputer systems my customers purchase.   Nothing else even comes
   close.  My words - Fortran still pays my salary.
   2. Why is that?   The math has not changed.   Open up those codes from
   #1 and most of them are doing pretty much the same thing -
   solving simultaneous partial differentials with a lot of unknowns.   What
   has changed is the size of the data sets, how to generate them and how
   manipulate them.   But the guts of the code, be it weather, bio, chem,
   physics, *etc*.. its the same as it was years ago.
   3. As importantly, the Chief Metallurgist for the US Gov at NIST is a
   good friend of mine.   As Dr. Fekete says -- the problem is we have years
   worth of data that has been checked and worked on with those codes, if we
   throw them out, we have revalidate the codes and the data with them.  I can
   not afford to do that.


So until you can create a new system that is not only blazing fast to do
the new job, you have to be able to go back and revalidate all the old
datasets too.   That's going to be even harder.    That's not an excuse to
not try mind you, but the economics are not in your favor.

That said, Moore's law is not going to help the way it did before.  So, to
continue to give people 'speed ups' something has to change.   The problem
is can we afford to change the code base and the data too?  I don't think
that is likely unless something really, really disruptive happens.

And that's is the problem.  In the high end,  we have never had a "
Christensen   Style" disruption.  Remember, a true Christensen disruption
starts off as a 'worse' technology that a new (and different) group of
people care about which don't care that 'sucks' compare to the established
technology.   It is that new market that makes it valuable, but it grows so
fast that  it eventually over takes the old market.

So far it really has not happened in the high end  in my career because
there has never been a 'new group'  that has cared about that style of
computing. The high end is the same folks as it has been since the 1940s.

Maybe a new group will appear in my children's time, but I suspect I will
not be here to see that occur as I just don't see anyone on the horizon
that I think could become such.

Clem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://minnie.tuhs.org/pipermail/tuhs/attachments/20170901/2033f00c/attachment.html>

