From: ron@ronnatalie.com (Ron Natalie)
Date: Mon, 6 Nov 2017 19:36:59 -0500
Subject: [TUHS] origins of void* -- Apology!
In-Reply-To: <fa31cbb0-c3a3-7c4a-1ec3-a59d06ecb5d9@kilonet.net>
References: <CANCZdfoLzdp6q4VsXo+cZ_gwMhyRxk2FT-jh2Dz2Ggt5pQdfJg@mail.gmail.com>
 <7617c69abf46fbe3f206c6208000fe3b26854359@webmail.yaccman.com>
 <065d01d3575e$f71f6ad0$e55e4070$@ronnatalie.com>
 <fa31cbb0-c3a3-7c4a-1ec3-a59d06ecb5d9@kilonet.net>
Message-ID: <066d01d35760$85ec7720$91c56560$@ronnatalie.com>

It’s worse than that.   “char” is defined as neither signed nor unsigned.   The signedness is implementation defined.    This was why we have the inane “signed” keyword.

 

 

From: TUHS [mailto:tuhs-bounces@minnie.tuhs.org] On Behalf Of Arthur Krewat
Sent: Monday, November 6, 2017 7:35 PM
To: tuhs at minnie.tuhs.org
Subject: Re: [TUHS] origins of void* -- Apology!

 

char (at least these days) is signed. So really, it's 7-bit ASCII.

I've been bitten by the 7-bit ASCII thing when it comes to modern character sets. unsigned char gets tiresome ;)



On 11/6/2017 7:25 PM, Ron Natalie wrote:

I believe one of C’s biggest failings is that they did not solve the schizophrenic definition of char*.

 

Char* as historically implemented and then  CODIFIED in the C and C++ standards is both the basic character type as well as the smallest addressable unit of storage.

This was all peachy keen in the 8 bit ASCII days (and even earlier alternative character sets such as EBCDIC, and its predecessors and other historical character sets like UNIVAC’s fielddata), but fell apart when we started into the 16 bit and larger UNICODE.

 

We needed a basic memory type that had sizeof == 1 (which void*) did not meet and release char from having to play double duty.

 

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://minnie.tuhs.org/pipermail/tuhs/attachments/20171106/b89f672d/attachment.html>

