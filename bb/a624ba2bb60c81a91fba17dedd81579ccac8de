Return-Path: <tuhs-bounces@minnie.tuhs.org>
X-Spam-Checker-Version: SpamAssassin 3.4.2 (2018-09-13) on inbox.vuxu.org
X-Spam-Level: 
X-Spam-Status: No, score=-1.1 required=5.0 tests=DKIM_SIGNED,DKIM_VALID,
	DKIM_VALID_AU,HEADER_FROM_DIFFERENT_DOMAINS,MAILING_LIST_MULTI,
	RCVD_IN_DNSWL_NONE autolearn=ham autolearn_force=no version=3.4.2
Received: from minnie.tuhs.org (minnie.tuhs.org [45.79.103.53])
	by inbox.vuxu.org (OpenSMTPD) with ESMTP id f365f63d
	for <ml@inbox.vuxu.org>;
	Fri, 31 May 2019 00:50:28 +0000 (UTC)
Received: by minnie.tuhs.org (Postfix, from userid 112)
	id 8C1A59B826; Fri, 31 May 2019 10:50:27 +1000 (AEST)
Received: from minnie.tuhs.org (localhost [127.0.0.1])
	by minnie.tuhs.org (Postfix) with ESMTP id 514B09B680;
	Fri, 31 May 2019 10:50:04 +1000 (AEST)
Authentication-Results: minnie.tuhs.org;
	dkim=pass (1024-bit key; unprotected) header.d=kilonet.net header.i=@kilonet.net header.b="H2Wsz3+G";
	dkim-atps=neutral
Received: by minnie.tuhs.org (Postfix, from userid 112)
 id 145749B67F; Fri, 31 May 2019 10:50:02 +1000 (AEST)
X-Greylist: delayed 439 seconds by postgrey-1.36 at minnie.tuhs.org;
 Fri, 31 May 2019 10:50:01 AEST
Received: from p3plsmtpa07-09.prod.phx3.secureserver.net
 (p3plsmtpa07-09.prod.phx3.secureserver.net [173.201.192.238])
 by minnie.tuhs.org (Postfix) with ESMTPS id 619D99B67F
 for <tuhs@minnie.tuhs.org>; Fri, 31 May 2019 10:50:01 +1000 (AEST)
Received: from medusa.kilonet.net ([72.69.11.12]) by :SMTPAUTH: with ESMTPA
 id WVd3hlGuVhFEoWVd4hWyUB; Thu, 30 May 2019 17:42:42 -0700
Received: from [199.89.231.101] (ender.kilonet.net [199.89.231.101])
 by medusa.kilonet.net (8.14.8/8.15.1) with ESMTP id x4V0gfIa020943
 for <tuhs@minnie.tuhs.org>; Thu, 30 May 2019 20:42:41 -0400 (EDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kilonet.net;
 s=default; t=1559263361;
 bh=itqLHMVjEVC8TxdUg+EmxeqakAlRtN+J25g9Bij4hs8=;
 h=Subject:To:References:From:Date:In-Reply-To;
 b=H2Wsz3+GRyq0FFObmrjxyAozMYiVu8kKAAT57j8FzDZbboJ0t5sVr/Ac50X//3znh
 y/yGY6laLewtztQIkacyD4R0gx7azpFEs/CozQQLHYGueRBUDaQLpt5PmKkJPLhDDJ
 UL1QDNw8Nu/QVS7k1OB9HJxJLnCT/HEkZ1FuF4PI=
To: tuhs@minnie.tuhs.org
References: <CMM.0.95.0.1559262062.beebe@gamma.math.utah.edu>
From: Arthur Krewat <krewat@kilonet.net>
Message-ID: <8c0e17fb-41e3-753c-9678-04e410825dce@kilonet.net>
Date: Thu, 30 May 2019 20:42:33 -0400
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101
 Thunderbird/60.6.1
MIME-Version: 1.0
In-Reply-To: <CMM.0.95.0.1559262062.beebe@gamma.math.utah.edu>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
Content-Language: en-US
X-CMAE-Envelope: MS4wfGs2LHw+C8EB7JVDYLcQ36l5ARv04C1GBVDL2oE9/0thG4IlUVZVcCnENBGsS8su3Gidpyq4i8/nFMzkUXfzGAD3OaaWcSsTv5mEplQoD5F5Hku/v4gA
 n8opRJdUdjH88Be9du6Yi8kGGrgeZxBs1UF2qqDyJMB5fJI+gJJi+wXb
Subject: Re: [TUHS] Quotas - did anyone ever use them?
X-BeenThere: tuhs@minnie.tuhs.org
X-Mailman-Version: 2.1.26
Precedence: list
List-Id: The Unix Heritage Society mailing list <tuhs.minnie.tuhs.org>
List-Unsubscribe: <https://minnie.tuhs.org/cgi-bin/mailman/options/tuhs>,
 <mailto:tuhs-request@minnie.tuhs.org?subject=unsubscribe>
List-Archive: <http://minnie.tuhs.org/pipermail/tuhs/>
List-Post: <mailto:tuhs@minnie.tuhs.org>
List-Help: <mailto:tuhs-request@minnie.tuhs.org?subject=help>
List-Subscribe: <https://minnie.tuhs.org/cgi-bin/mailman/listinfo/tuhs>,
 <mailto:tuhs-request@minnie.tuhs.org?subject=subscribe>
Errors-To: tuhs-bounces@minnie.tuhs.org
Sender: "TUHS" <tuhs-bounces@minnie.tuhs.org>

On 5/30/2019 8:21 PM, Nelson H. F. Beebe wrote:
> Several list members report having used, or suffered under, filesystem
> quotas.
>
> At the University Utah, in the College of Science, and later, the
> Department of Mathematics, we have always had an opposing view:
>
> 	Disk quotas are magic meaningless numbers imposed by some bozo
> 	ignorant system administrator in order to prevent users from
> 	getting their work done.

You've never had people like me on your systems ;) - But yeah...

> For the last 15+ years, our central fileservers have run ZFS on
> Solaris 10 (SPARC, then on Intel x86_64), and for the last 17 months,
> on GNU/Linux CentOS 7.
>
I do the same with ZFS - limit the individual filesystems with "zfs set 
quota=xxx" so the entire pool can't be filled. I assign a zfs filesystem 
to an individual user in /export/home and when they need more, they let 
me know. Various monitoring scripts tell me when a filesystem is 
approaching 80%, and I either just expand it on my own because of the 
user's usage, or let them know they are approaching the limit.

Same thing with Netbackup Basic Disk pools in a common ZFS pool. I can 
adjust them as needed, and Netbackup sees the change almost immediately.

At home, I did this with my kids ;) - Samba and zfs quota on the 
filesystem let them know how much room they had.

art k.

PS: I'm starting to move to FreeBSD and ZFS for VMware datastores, the 
performance is outstanding over iSCSI on 10Gbe - (which Solaris 11's 
COMSTAR is not apparently very good at especially with small block 
sizes). I have yet to play with Linux and ZFS but would appreciate to 
hear (privately, if it's not appropriate for the list) your experiences 
with it.


